#LLM specific config
#Training.example config essentially contains data mixed together
#e.g.
#model_name: "bert-base-uncased"
#learning_rate: 0.001
#batch_size: 16
#epochs: 10
#max_length: 512
#data_path: "./data/raw"
#tree_max_depth: 10

#However we need one for the llm component